{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743a875c",
   "metadata": {},
   "source": [
    "# Importing Header and SoilPrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0532e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Header.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb \n",
    "from Header import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e341a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from SoilPrep.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from SoilPrep import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38ffb5",
   "metadata": {},
   "source": [
    "## Loading Data and MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af371245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from LoadDataMetaData.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nbformat\\__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "  validate(nb)\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from LoadDataMetaData import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba215bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('say \"your Meta Tree started building\"')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639a3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_spec = ['none', 'fod2', 'continuum', 'log']\n",
    "# prepare_target = ['none', 'minmax']\n",
    "\n",
    "def find_X(p, n):\n",
    "    if p == 'fod':\n",
    "        X = fod_sampled[n]\n",
    "    elif p == 'cr':\n",
    "        X = sampled_cr[n]\n",
    "    elif p == 'log':\n",
    "        X = sampled_log[n]\n",
    "    elif p == 'none':\n",
    "        X = sampled_spec[n]\n",
    "    elif p == 'fod_cr':\n",
    "        X = fod_cr[n]\n",
    "    else:\n",
    "        X = fod_log[n]\n",
    "    return X\n",
    "\n",
    "def find_spec(p, n, m):\n",
    "    if n == 0:\n",
    "        if m == 'randomforest' or m == 'cubist' or m == 'gbrt':\n",
    "            spec = find_X(p, 100)\n",
    "        else:\n",
    "            spec = find_X(p,n)            \n",
    "    else:\n",
    "        spec = find_X(p,n)\n",
    "    return spec    \n",
    "\n",
    "\n",
    "def find_y(t):\n",
    "    i = target_names.index(t) \n",
    "    y = T[i]\n",
    "    return y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d061d551-ba6e-4ee8-b891-279c8a831c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\abhis\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\_pywrap_tensorflow_internal.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached tensorflow-2.16.1-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-intel==2.16.1 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached h5py-3.11.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached grpcio-1.64.1-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached optree-0.11.0-cp311-cp311-win_amd64.whl.metadata (46 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\abhis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.16.1-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Using cached tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl (377.0 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.64.1-cp311-cp311-win_amd64.whl (4.1 MB)\n",
      "Using cached h5py-3.11.0-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "Using cached keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl (127 kB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.11.0-cp311-cp311-win_amd64.whl (245 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, optree, markdown-it-py, rich, keras, tensorflow-intel, tensorflow\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a343e3ce-d96e-40a3-bf3d-013b5f71787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1214a5b5-db21-4df9-ac37-d920814b9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#halt here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6552f5c7-b615-43ce-b93e-730943346e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Collection and Preprocessing\n",
    "# Assume hyperspectral_data is a NumPy array of shape (n_samples, n_features)\n",
    "# Assume soil_attributes is a NumPy array of shape (n_samples, n_targets)\n",
    "\n",
    "# Load your data here\n",
    "# hyperspectral_data = ...\n",
    "# soil_attributes = ...\n",
    "\n",
    "# Example synthetic data (for demonstration purposes)\n",
    "#np.random.seed(42)\n",
    "#hyperspectral_data = np.random.rand(1000, 100)  # 1000 samples, 100 features\n",
    "hyperspectral_data = spec2[  # 930 samples, 100 features\n",
    "soil_attributes =  find_y('Clay')    # 1000 samples, 1 target attributes\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(hyperspectral_data, soil_attributes, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "495e28c5-fe72-40c6-af15-e88aad7f7a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 2. Model Design\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)  # Output layer size should match the number of target attributes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59913ac9-5031-4619-b163-38f633146590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - loss: 612.1364 - r2_score: -1.7390 - val_loss: 279.0638 - val_r2_score: -0.1752\n",
      "Epoch 2/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 286.2695 - r2_score: -0.1947 - val_loss: 246.0524 - val_r2_score: -0.0362\n",
      "Epoch 3/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 228.8546 - r2_score: 0.0242 - val_loss: 225.2956 - val_r2_score: 0.0512\n",
      "Epoch 4/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 197.7541 - r2_score: 0.1601 - val_loss: 188.9866 - val_r2_score: 0.2041\n",
      "Epoch 5/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 173.7972 - r2_score: 0.2752 - val_loss: 159.3565 - val_r2_score: 0.3289\n",
      "Epoch 6/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 184.1690 - r2_score: 0.2228 - val_loss: 143.9244 - val_r2_score: 0.3939\n",
      "Epoch 7/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 149.5652 - r2_score: 0.3579 - val_loss: 138.9547 - val_r2_score: 0.4148\n",
      "Epoch 8/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 130.2641 - r2_score: 0.4505 - val_loss: 127.0260 - val_r2_score: 0.4651\n",
      "Epoch 9/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 127.3539 - r2_score: 0.4250 - val_loss: 145.9413 - val_r2_score: 0.3854\n",
      "Epoch 10/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 130.3936 - r2_score: 0.4265 - val_loss: 140.3084 - val_r2_score: 0.4091\n",
      "Epoch 11/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 125.1299 - r2_score: 0.4549 - val_loss: 175.5394 - val_r2_score: 0.2608\n",
      "Epoch 12/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 129.7114 - r2_score: 0.4446 - val_loss: 120.6063 - val_r2_score: 0.4921\n",
      "Epoch 13/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 109.8288 - r2_score: 0.5343 - val_loss: 113.1270 - val_r2_score: 0.5236\n",
      "Epoch 14/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 111.8466 - r2_score: 0.5287 - val_loss: 107.5776 - val_r2_score: 0.5470\n",
      "Epoch 15/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 102.3895 - r2_score: 0.5702 - val_loss: 111.4049 - val_r2_score: 0.5309\n",
      "Epoch 16/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 96.7168 - r2_score: 0.5933 - val_loss: 126.8113 - val_r2_score: 0.4660\n",
      "Epoch 17/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 100.4613 - r2_score: 0.5539 - val_loss: 129.1565 - val_r2_score: 0.4561\n",
      "Epoch 18/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 104.5777 - r2_score: 0.5253 - val_loss: 104.8637 - val_r2_score: 0.5584\n",
      "Epoch 19/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 89.1096 - r2_score: 0.6115 - val_loss: 128.9726 - val_r2_score: 0.4569\n",
      "Epoch 20/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 105.8875 - r2_score: 0.5344 - val_loss: 100.9412 - val_r2_score: 0.5749\n",
      "Epoch 21/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 88.7616 - r2_score: 0.6115 - val_loss: 88.7757 - val_r2_score: 0.6261\n",
      "Epoch 22/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 93.4769 - r2_score: 0.5655 - val_loss: 134.0765 - val_r2_score: 0.4354\n",
      "Epoch 23/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 94.5336 - r2_score: 0.5867 - val_loss: 97.2794 - val_r2_score: 0.5903\n",
      "Epoch 24/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 97.4834 - r2_score: 0.5937 - val_loss: 102.8228 - val_r2_score: 0.5670\n",
      "Epoch 25/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 98.6942 - r2_score: 0.5931 - val_loss: 106.4987 - val_r2_score: 0.5515\n",
      "Epoch 26/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 90.5833 - r2_score: 0.6261 - val_loss: 90.7090 - val_r2_score: 0.6180\n",
      "Epoch 27/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 78.3031 - r2_score: 0.6521 - val_loss: 92.2805 - val_r2_score: 0.6114\n",
      "Epoch 28/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 68.4267 - r2_score: 0.6975 - val_loss: 82.7471 - val_r2_score: 0.6515\n",
      "Epoch 29/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 69.7833 - r2_score: 0.6876 - val_loss: 108.0610 - val_r2_score: 0.5449\n",
      "Epoch 30/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 82.8205 - r2_score: 0.6478 - val_loss: 91.9634 - val_r2_score: 0.6127\n",
      "Epoch 31/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 69.7579 - r2_score: 0.6955 - val_loss: 137.0585 - val_r2_score: 0.4228\n",
      "Epoch 32/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 84.0965 - r2_score: 0.6107 - val_loss: 85.2645 - val_r2_score: 0.6409\n",
      "Epoch 33/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 63.3532 - r2_score: 0.7345 - val_loss: 101.1512 - val_r2_score: 0.5740\n",
      "Epoch 34/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 87.2388 - r2_score: 0.6029 - val_loss: 91.8531 - val_r2_score: 0.6132\n",
      "Epoch 35/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 64.7192 - r2_score: 0.7131 - val_loss: 88.8293 - val_r2_score: 0.6259\n",
      "Epoch 36/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 59.9832 - r2_score: 0.7380 - val_loss: 90.1426 - val_r2_score: 0.6204\n",
      "Epoch 37/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 65.9945 - r2_score: 0.7065 - val_loss: 82.4072 - val_r2_score: 0.6530\n",
      "Epoch 38/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 65.1263 - r2_score: 0.7151 - val_loss: 74.0270 - val_r2_score: 0.6883\n",
      "Epoch 39/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 49.8152 - r2_score: 0.7921 - val_loss: 85.5293 - val_r2_score: 0.6398\n",
      "Epoch 40/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 52.3699 - r2_score: 0.7738 - val_loss: 81.4239 - val_r2_score: 0.6571\n",
      "Epoch 41/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 56.5089 - r2_score: 0.7497 - val_loss: 96.1490 - val_r2_score: 0.5951\n",
      "Epoch 42/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 80.1629 - r2_score: 0.6423 - val_loss: 118.5729 - val_r2_score: 0.5007\n",
      "Epoch 43/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 70.2146 - r2_score: 0.7020 - val_loss: 88.3228 - val_r2_score: 0.6281\n",
      "Epoch 44/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 71.7054 - r2_score: 0.6839 - val_loss: 67.1484 - val_r2_score: 0.7172\n",
      "Epoch 45/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 48.4587 - r2_score: 0.7788 - val_loss: 98.5006 - val_r2_score: 0.5852\n",
      "Epoch 46/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 57.1608 - r2_score: 0.7483 - val_loss: 88.0747 - val_r2_score: 0.6291\n",
      "Epoch 47/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 59.4396 - r2_score: 0.7445 - val_loss: 76.8300 - val_r2_score: 0.6765\n",
      "Epoch 48/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 48.2220 - r2_score: 0.7982 - val_loss: 70.2322 - val_r2_score: 0.7042\n",
      "Epoch 49/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 51.5915 - r2_score: 0.7735 - val_loss: 70.3720 - val_r2_score: 0.7037\n",
      "Epoch 50/50\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 52.4722 - r2_score: 0.7774 - val_loss: 78.4254 - val_r2_score: 0.6697\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 74.6583 - r2_score: 0.6639 \n",
      "Test Loss: 75.8404312133789\n",
      "Test R2: 0.6631842851638794\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[[12.310964]\n",
      " [16.774666]\n",
      " [23.520292]\n",
      " [17.194622]\n",
      " [ 9.984883]]\n"
     ]
    }
   ],
   "source": [
    "# 3. Model Training\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['r2_score'])\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 4. Model Evaluation\n",
    "loss, r2 = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test R2: {r2}\")\n",
    "\n",
    "# 5. Prediction\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Example output of predictions\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3d315",
   "metadata": {},
   "source": [
    "## Recording Hypertuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "model ={}\n",
    "param_grid ={}\n",
    "model['mult'] = LinearRegression()\n",
    "param_grid['mult'] = { 'fit_intercept': [True, False] }\n",
    "\n",
    "model['ridge'] = KernelRidge()\n",
    "param_grid['ridge']={'alpha': [ 0.00001,0.00002, 0.00005, 0.0001, 0.0002, 0.0005, 0.001, 0.002, \\\n",
    "                               0.005,  0.01, 0.05, 0.1, 0.5, 1]}\n",
    "\n",
    "model['plsr'] = PLSRegression()\n",
    "param_grid['plsr']={'n_components': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "\n",
    "model['cubist'] = Cubist(random_state = 42)  \n",
    "param_grid['cubist'] = { 'n_committees': [5, 10, 15, 20], 'n_rules': [10, 20, 30, 40, 50] }\n",
    "                        \n",
    "\n",
    "model['gbrt'] = GradientBoostingRegressor(random_state = 42)\n",
    "param_grid['gbrt'] = {\n",
    "    'n_estimators': [30,40,50],        # Number of boosting stages to be used\n",
    "    'learning_rate': [0.01, 0.1],     # Step size shrinkage used in updating weights\n",
    "    'max_depth': [3, 4, 5]                # Maximum depth of individual trees\n",
    "#     'min_samples_split': [2, 3],        # Minimum number of samples required to split an internal node\n",
    "#     'min_samples_leaf': [1, 2],         # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "model['svr'] = SVR()\n",
    "param_grid['svr'] = {\n",
    "    'C': [0.1, 1, 10],              # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],   # Kernel function\n",
    "    'gamma': [0.01, 0.1, 1],      # Kernel coefficient (only for 'rbf' kernel)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7e396",
   "metadata": {},
   "source": [
    "### Finding Hypertuned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb697712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_core_model_param_on (m, t, p, tp, n):\n",
    "#     Model = model[m]\n",
    "#     X = find_spec(p,n,m)\n",
    "#     y = find_y(t, tp)\n",
    "    \n",
    "#     row, col = X.shape\n",
    "\n",
    "#     scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "#     cv = RepeatedKFold(n_splits=row, n_repeats=1, random_state=3)\n",
    "        \n",
    "#     grid_search = GridSearchCV(Model, param_grid=param_grid[m], cv=cv,\\\n",
    "#                                 scoring= scorer, refit=True, verbose=1, error_score='raise', n_jobs=-1)\n",
    "#     grid_search.fit(X,y)\n",
    "    \n",
    "#     return (grid_search.best_params_)\n",
    "\n",
    "def find_tuned_param_on (X_train, y_train, m):\n",
    "    Model = model[m]\n",
    "    X = X_train\n",
    "    y = y_train\n",
    "    \n",
    "    row, col = X.shape\n",
    "\n",
    "    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "    cv = RepeatedKFold(n_splits=row, n_repeats=1, random_state=3)\n",
    "        \n",
    "    grid_search = GridSearchCV(Model, param_grid=param_grid[m], cv=cv,\\\n",
    "                                scoring= scorer, refit=True, verbose=1, error_score='raise', n_jobs=-1)\n",
    "    grid_search.fit(X,y)\n",
    "    \n",
    "    return (grid_search.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b259e6c",
   "metadata": {},
   "source": [
    "### Creating Hypertuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_core_model_on (m, BP):\n",
    "    if m == 'mult':\n",
    "        Model = LinearRegression()\n",
    "        \n",
    "    elif m == 'ridge':\n",
    "        Model =  KernelRidge(alpha = BP['alpha'])\n",
    "        \n",
    "    elif m == 'plsr':\n",
    "        Model = PLSRegression(n_components = BP['n_components'])\n",
    "        \n",
    "    elif m == 'cubist':\n",
    "        Model =  Cubist( n_committees = BP['n_committees'], n_rules = BP['n_rules'], random_state = 42) \n",
    "    \n",
    "    elif m == 'gbrt':\n",
    "        Model = GradientBoostingRegressor(n_estimators = BP['n_estimators'], learning_rate = BP['learning_rate'], \\\n",
    "                                          max_depth = BP['max_depth'],  random_state = 42)\n",
    "    elif m == 'svr':\n",
    "        Model = SVR(C = BP['C'], kernel = BP['kernel'], gamma = BP['gamma'])\n",
    "        \n",
    "    return Model    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4cd13c",
   "metadata": {},
   "source": [
    "## Building HMtree and BPtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc269acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_methods = ['gbrt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8544989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_y_pred (spectra, target, method, bp):\n",
    "    m = method\n",
    "    (r,c) = spectra.shape\n",
    "    \n",
    "    Y_test = target.copy()\n",
    "    Y_pred = target.copy()\n",
    "    #print('r:', r)\n",
    "    for i in range (0, r): \n",
    "        #print('i: ', i)\n",
    "        full_spec = spectra.copy()\n",
    "        X_train = full_spec.drop(full_spec.index[i], axis=0)\n",
    "        X_test = full_spec.iloc[[i],:].copy()\n",
    "        full_tar = target.copy()\n",
    "        y_train = full_tar.drop(full_tar.index[i], axis=0)\n",
    "        y_test = full_tar.iloc[i].copy()        \n",
    "        #---- Model Creation, fitting, and predictions--------\n",
    "        Model = create_core_model_on (m, bp)\n",
    "        Model.fit(X_train, y_train)\n",
    "        y_pred = Model.predict(X_test)\n",
    "        Y_pred.iloc[i] = y_pred\n",
    "        \n",
    "    return Y_pred      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206db504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_HMtree_on (method_name, target):\n",
    "    \n",
    "    tree ={}\n",
    "    \n",
    "    start = time.time()\n",
    "    m = method_name\n",
    "    t = target\n",
    "    \n",
    "    print('tree for: '+ m +' ------> running on: ' + t)\n",
    "    \n",
    "    for tp in prepare_target:\n",
    "        tree[tp] ={}        \n",
    "        for p in prepare_spec:\n",
    "            tree[tp][p] ={}            \n",
    "            for n in nbands_sampling:\n",
    "                tree[tp][p][n] ={}\n",
    "                Y = tree[tp][p][n]\n",
    "                                           \n",
    "                #------ setting spec to appropriate (sampled) spectra----                \n",
    "                spec = find_spec(p, n, m)                \n",
    "                                        \n",
    "                #---- target selection and normalization ---\n",
    "                y = find_y(t)\n",
    "                                \n",
    "                #---- performing train-test split----------------------\n",
    "                X_train, X_test, y_train, y_test = train_test_split(spec, y, test_size= 0.3, random_state=42)\n",
    "                            \n",
    "                #----- hypertuning parameter, model creation, fitting and prediction ----                \n",
    "                \n",
    "                #bp = find_tuned_param_on (X_train, y_train, m)\n",
    "                bp = find_tuned_param_on (spec, y, m)\n",
    "                \n",
    "                Y['bp'] = bp\n",
    "                print('Method: '+ m + '---> Target: '+ t+ '----> SpecProc: ' +p+ ' ---> n_band: ', n)\n",
    "                Model = create_core_model_on (m, bp)\n",
    "                Model.fit(X_train, y_train)\n",
    "                \n",
    "                y_pred = Model.predict(X_test)\n",
    "                yhat_pred = Model.predict(X_train)\n",
    "                    \n",
    "                #----- Data for Model accuracy and plotting -----------\n",
    "                Y['X_test'] = X_test                \n",
    "                Y['y_test'] = y_test\n",
    "                Y['y_testP'] = y_pred\n",
    "                \n",
    "                Y['X_train'] = X_train\n",
    "                Y['y_train'] = y_train\n",
    "                Y['y_trainP'] = yhat_pred\n",
    "                    \n",
    "                Y['iqrp_test'] = find_iqrp(y_pred, y_test)\n",
    "                Y['r2_test'] = find_r2(y_pred, y_test)\n",
    "                Y['rpd_test'] = find_rpd(y_pred, y_test)                 \n",
    "                \n",
    "                Y['r2_train'] = find_r2(yhat_pred, y_train)\n",
    "                \n",
    "                #------- L1 out prediction on test data -----------------\n",
    "                L1y_pred = L1_y_pred (spec, y, m, bp)\n",
    "                Y['L1y_testP'] = L1y_pred\n",
    "                Y['L1y_test'] = y\n",
    "                 \n",
    "                \n",
    "                Y['L1iqrp_test'] = find_iqrp(L1y_pred, y)\n",
    "                Y['L1r2_test'] = find_r2(L1y_pred, y)\n",
    "                Y['L1rpd_test'] = find_rpd(L1y_pred, y) \n",
    "                \n",
    "                                                                         \n",
    "    end = time.time()                            \n",
    "    os.system('say \"your hyper tuned meta tree finished for one more method\"')                            \n",
    "\n",
    "    print('End time - Start time =', (end-start)) \n",
    "    \n",
    "    return tree.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b5a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_HMtree_for (method_name):\n",
    "    tree ={}\n",
    "    \n",
    "    start = time.time()\n",
    "    m = method_name\n",
    "    for t in target_names:\n",
    "        #print('tree for: '+ m +' ------> running on: ' + t)\n",
    "        tree[t] = build_HMtree_on (m, t)\n",
    "        \n",
    "    end = time.time()                            \n",
    "    os.system('say \"your meta tree finished for one more method\"')                            \n",
    "    print('End time - Start time =', (end-start))     \n",
    "    return tree.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ffdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_score_on (HMtree, target, method, scorer):\n",
    "    t=target\n",
    "    m= method\n",
    "    \n",
    "    best_score = -1\n",
    "    #best_n_comp = 'NA'\n",
    "    \n",
    "    \n",
    "    for tp in prepare_target:\n",
    "        for p in prepare_spec:\n",
    "            for n in nbands_sampling:\n",
    "                Y = HMtree[m][t][tp][p][n]\n",
    "                #r2_train = Y['r2_train']\n",
    "                #r2_test = Y['r2_test']\n",
    "                #L1r2_test = Y['L1r2_test']\n",
    "                 \n",
    "                    \n",
    "                if scorer == 'iqrp':\n",
    "                    cur_score = Y['iqrp_test']\n",
    "                elif scorer == 'L1iqrp':\n",
    "                    cur_score = Y['L1iqrp_test']\n",
    "                elif scorer == 'r2':\n",
    "                    cur_score = Y['r2_test'] \n",
    "                else:\n",
    "                    cur_score = Y['L1r2_test']  \n",
    "                    \n",
    "                if cur_score > best_score:\n",
    "                    best_score = cur_score\n",
    "                    best_tp = tp\n",
    "                    best_n = n\n",
    "                    best_p = p\n",
    "                            \n",
    "    param_list = [scorer, np.round(best_score,2), 'Spec:', best_p, 'bands:', best_n]                                 \n",
    "    return (param_list)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_score_for (HMtree, target, scorer):\n",
    "    \n",
    "    for method in ml_methods:\n",
    "        param_list= best_score_on (HMtree, target, method, scorer)\n",
    "        print('For:'+target+'->', param_list, ':'+method)\n",
    "     \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open ('HMtree.pickle', 'rb') as file:\n",
    "#     HMtree = pickle.load(file)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e685fd",
   "metadata": {},
   "source": [
    "## Creating different branches of Mtree (for separate methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d90962",
   "metadata": {},
   "outputs": [],
   "source": [
    "HMtree = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5b155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweta\\AppData\\Roaming\\Python\\Python38\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  20\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  21\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  23\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  25\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  27\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  29\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  30\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  31\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  33\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  35\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  37\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  39\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  40\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  45\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  50\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  55\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  60\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  70\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  80\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  90\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_cr ---> n_band:  100\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  0\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  2\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  3\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  5\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  7\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  9\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  10\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  11\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  13\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  15\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  17\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  19\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  20\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  21\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  23\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  25\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  27\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  29\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  30\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  31\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  33\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  35\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  37\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  39\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  40\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  45\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  50\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  55\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  60\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  70\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  80\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  90\n",
      "Fitting 934 folds for each of 18 candidates, totalling 16812 fits\n",
      "Method: gbrt---> Target: Clay----> SpecProc: fod_log ---> n_band:  100\n",
      "End time - Start time = 197012.2972407341\n",
      "End time - Start time = 588741.2253873348\n"
     ]
    }
   ],
   "source": [
    "HMtree['gbrt'] = build_HMtree_for('gbrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b134a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('HMtreeGbrt.pickle', 'wb') as file:\n",
    "    pickle.dump(HMtree, file)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Model Accuracy (ipywidgets)\n",
    "def plot_model_acc (method, target, spec_preprocessing, n_bands):\n",
    "    \n",
    "    m = method\n",
    "    t = target\n",
    "    \n",
    "    i = target_names.index(target)    \n",
    "    \n",
    "    p = spec_preprocessing\n",
    "    n = n_bands\n",
    "    tp = 'none'\n",
    "    \n",
    "    Y = HMtree[m][t][tp][p][n]\n",
    "    \n",
    "    y_test = Y['y_test']\n",
    "    y_pred = Y['y_testP']\n",
    "    L1y_pred = Y['L1y_testP']\n",
    "    L1y_test = Y['L1y_test']\n",
    "    \n",
    "\n",
    "    \n",
    "    if m == 'plsr':\n",
    "        y_pred = y_pred[:,0]\n",
    "        #L1y_pred = L1y_pred[:,0]\n",
    "\n",
    "    \n",
    "    iqrp_test = Y['iqrp_test']\n",
    "    r2_test = Y['r2_test']\n",
    "    rpd_test = Y['rpd_test']\n",
    "    \n",
    "    L1iqrp_test = Y['L1iqrp_test']\n",
    "    L1r2_test = Y['L1r2_test']\n",
    "    L1rpd_test = Y['L1rpd_test']\n",
    "    \n",
    "    y_tp = pd.DataFrame({'actual':y_test.values, 'predic': y_pred})\n",
    "    z = np.polyfit(y_test, y_pred, 1)\n",
    "    \n",
    "    L1y_tp = pd.DataFrame({'actual':L1y_test.values, 'predic': L1y_pred})\n",
    "    L1z = np.polyfit(L1y_test, L1y_pred, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2,2, figsize=(18,16))\n",
    "    \n",
    "    #with plt.style.context(('ggplot')): ---- PLOT of test-prediction --------------------------------------\n",
    "    y_tp.plot.scatter(ax= axes[0][0], x=\"actual\", y=\"predic\", alpha=0.8, color = clr[i], edgecolors='k')\n",
    "    axes[0][0].plot(y_test, np.polyval(z, y_test),  c='blue', linewidth=1)\n",
    "    axes[0][0].plot(y_test, y_test, color='green', linewidth=1)\n",
    "    axes[0][0].tick_params(axis='both', labelsize=10)\n",
    "    axes[0][0].text(0.05, 0.95, target_names[i]+' (Train-Test Score)', transform=axes[0][0].transAxes, fontsize = 20, color = clr[i])\n",
    "    axes[0][0].text(0.05, 0.90, 'IQRP ={:.2f}'.format(iqrp_test), transform=axes[0][0].transAxes, fontsize = 16)\n",
    "    axes[0][0].text(0.05, 0.85, 'RPD ={:.2f}'.format(rpd_test), transform=axes[0][0].transAxes, fontsize = 16)\n",
    "    axes[0][0].text(0.05, 0.80, 'R2 ={:.2f}'.format(np.round(r2_test,3)), transform=axes[0][0].transAxes, fontsize = 16)\n",
    "    axes[0][0].text(0.95, 0.15, 'Method: '+method, transform=axes[0][0].transAxes, \n",
    "                    horizontalalignment='right', fontsize = 20)\n",
    "    \n",
    "    L1y_tp.plot.scatter(ax= axes[0][1], x=\"actual\", y=\"predic\", alpha=0.8, color = clr[i], edgecolors='k')\n",
    "    axes[0][1].plot(L1y_test, np.polyval(L1z, L1y_test),  c='blue', linewidth=1)\n",
    "    axes[0][1].plot(L1y_test, L1y_test, color='green', linewidth=1)\n",
    "    axes[0][1].tick_params(axis='both', labelsize=10)\n",
    "    axes[0][1].text(0.05, 0.95, target_names[i]+' (L1-out Score)', transform=axes[0][1].transAxes, fontsize = 20, color = clr[i])\n",
    "    axes[0][1].text(0.05, 0.90, 'IQRP ={:.2f}'.format(L1iqrp_test), transform=axes[0][1].transAxes, fontsize = 16)\n",
    "    axes[0][1].text(0.05, 0.85, 'RPD ={:.2f}'.format(L1rpd_test), transform=axes[0][1].transAxes, fontsize = 16)\n",
    "    axes[0][1].text(0.05, 0.80, 'R2 ={:.2f}'.format(np.round(L1r2_test,3)), transform=axes[0][1].transAxes, fontsize = 16)\n",
    "    axes[0][1].text(0.95, 0.15, 'Method: '+method, transform=axes[0][1].transAxes, \n",
    "                    horizontalalignment='right', fontsize = 20)\n",
    "\n",
    "    \n",
    "     #------------------------ Ploting R2 Vs N_bands-----------------------------------\n",
    "    \n",
    "    #--- data for creating n_bands vs r2 scores plot -------------------------------\n",
    "    \n",
    "    pY = HMtree[m][t]['none'][p]        \n",
    "    X = nbands_sampling\n",
    "    \n",
    "    Yr2 = []\n",
    "    L1Yr2 = []\n",
    "    \n",
    "    for j in X:\n",
    "        Yr2.append(pY[j]['r2_test'])\n",
    "        L1Yr2.append(pY[j]['L1r2_test'])  \n",
    "   \n",
    "    j = 0\n",
    "    while j < len(Yr2): \n",
    "        if Yr2[j] <= 0:\n",
    "            Yr2[j] = 0\n",
    "        if L1Yr2[j] <= 0:\n",
    "            L1Yr2[j] = 0\n",
    "        j = j + 1\n",
    " \n",
    "    #------------------------------ STEM PLOT ---- for accuracy Vs n_bands ---------------------------------- \n",
    "    \n",
    "    axes[1][0].stem(X,Yr2)\n",
    "    axes[1][0].tick_params(axis='both', labelsize=10)\n",
    "    axes[1][0].text(0.00, 1.01,  target_names[i], transform=axes[1][0].transAxes,fontsize = 20, color = clr[i])\n",
    "    axes[1][0].text(0.99, 1.01, 'Spec_prep: '+ p, transform=axes[1][0].transAxes, horizontalalignment='right', fontsize = 16)\n",
    "    axes[1][0].text(0.17, 1.01, '(train-test)', transform=axes[1][0].transAxes, horizontalalignment='left', fontsize = 16)\n",
    "    axes[1][0].text(0.60, -0.1, 'n_bands', transform=axes[1][0].transAxes, horizontalalignment='right', fontsize = 16)\n",
    "    axes[1][0].text(-0.1, 0.5, 'R2 Scores', horizontalalignment='left', verticalalignment='center', \\\n",
    "                rotation='vertical', transform=axes[1][0].transAxes, fontsize = 16)\n",
    "    \n",
    "    \n",
    "    axes[1][1].stem(X,L1Yr2)\n",
    "    axes[1][1].tick_params(axis='both', labelsize=10)\n",
    "    axes[1][1].text(0.00, 1.01,  target_names[i], transform=axes[1][1].transAxes,fontsize = 20, color = clr[i])\n",
    "    axes[1][1].text(0.99, 1.01, 'Spec_prep: '+ p, transform=axes[1][1].transAxes, horizontalalignment='right', fontsize = 16)\n",
    "    axes[1][1].text(0.17, 1.01, '(L1-out)', transform=axes[1][1].transAxes, horizontalalignment='left', fontsize = 16)\n",
    "    axes[1][1].text(0.60, -0.1, 'n_bands', transform=axes[1][1].transAxes, horizontalalignment='right', fontsize = 16)\n",
    "    axes[1][1].text(-0.1, 0.5, 'R2 Scores', horizontalalignment='left', verticalalignment='center', \\\n",
    "                rotation='vertical', transform=axes[1][1].transAxes, fontsize = 16)\n",
    "        \n",
    "   \n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4997300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml_methods = ['mult', 'ridge', 'plsr', 'svr', 'cubist', 'gbrt']\n",
    "\n",
    "#ml_methods = ['mult', 'plsr', 'svr', 'ridge', 'cubist','gbrt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score_for (HMtree, 'Sand', 'r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42821aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_for (HMtree, 'Sand', 'L1r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score_for (HMtree, 'Silt', 'r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec36473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_for (HMtree, 'Silt', 'L1r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score_for (HMtree, 'Clay', 'r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_for (HMtree, 'Clay', 'L1r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb323330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score_for (HMtree, 'TOC', 'r2')-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da0e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score_for (HMtree, 'TOC', 'L1r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score_for (HMtree, 'CaCO3', 'r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score_for (HMtree, 'CaCO3', 'L1r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abea8f",
   "metadata": {},
   "source": [
    "## Plotting Model Accuracy (ipywidgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipywidgets.interact( plot_model_acc, target = target_names, method = ml_methods, \\\n",
    "                    spec_preprocessing = prepare_spec,  \\\n",
    "                    target_preprocessing = prepare_target, n_bands = nbands_sampling )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76eccee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
