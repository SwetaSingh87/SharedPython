{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e5f066",
   "metadata": {},
   "source": [
    "# Importing Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdca4063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Header.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nbformat\\__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "  validate(nb)\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb \n",
    "from Header import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5652d7",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing (Spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f848583b",
   "metadata": {},
   "source": [
    "## Smoothing Filter ( First Order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e73ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input spectra as features\n",
    "\n",
    "def sgsmooth (spectrum,window):\n",
    "    filt_spec = spectrum.iloc[:,:].copy()\n",
    "    (row,col) = spectrum.shape\n",
    "    if row == 1:\n",
    "        print('you should have given spectra as a dataframe -- not series')\n",
    "    if window != 0:\n",
    "        for c in range (0,col):\n",
    "            if c>= window and c <= (col-window):      ## SAFE\n",
    "                filt_spec.iloc[:,c] = spectrum.iloc[:,(c-window):(c+window)].mean(axis=1).copy()\n",
    "            elif c < window and c <= (col-window):    ##  LEFT\n",
    "                filt_spec.iloc[:,c] = spectrum.iloc[:,0:(c+window)].mean(axis=1).copy()\n",
    "            elif c > (col-window) and c >= window:    ## RIGHT\n",
    "                filt_spec.iloc[:,c] = spectrum.iloc[:,(c-window):col].mean(axis=1).copy()\n",
    "            else:     ## LEFT & RIGHT  c < window and c > (col-window)\n",
    "                filt_spec.iloc[:,c] = spectrum.iloc[:, 0:col].mean(axis=1).copy()        \n",
    "    return (filt_spec.iloc[:,:].copy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8da222",
   "metadata": {},
   "source": [
    "## Smoothing Filter ( Savitzky Golay First and Second Order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0060f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- Smoothing Spectra using sg1: (savgol order 1) and sg2: (savgol order 2)  -----------\n",
    "\n",
    "#--INPUT: window_len (should be odd positive integer), filt_type (should be 'sg1' or 'sg2')\n",
    "\n",
    "\n",
    "def filt_sg(spectra, window_len, filt_type):\n",
    "    sg = filt_type\n",
    "    w = window_len\n",
    "    \n",
    "    if sg == 'sg1':\n",
    "        if w ==0 or w == 1:\n",
    "            smth_spec = spectra.copy()   \n",
    "        else:\n",
    "            smth_spec = spectra.copy()\n",
    "            pd.DataFrame(savgol_filter(smth_spec, w, 1, axis=1), columns=smth_spec.columns, index=smth_spec.index)\n",
    "            \n",
    "    else:\n",
    "        if w ==0 or w == 1:\n",
    "            smth_spec = spectra.copy()   \n",
    "        else:\n",
    "            smth_spec = spectra.copy()\n",
    "            pd.DataFrame(savgol_filter(smth_spec, w, 2, axis=1), columns=smth_spec.columns, index=smth_spec.index)\n",
    "            \n",
    "    return (smth_spec)\n",
    "\n",
    "#--OUTPUT: smoothed spectra with same column names and row indices as original (input) spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89e207",
   "metadata": {},
   "source": [
    "## First Order Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0683d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fod (spectra):\n",
    "    fo_spec = spectra.iloc[:,:].copy()\n",
    "    (row,col) = fo_spec.shape\n",
    "    \n",
    "    for i in range(0,col):\n",
    "        if i==col-1:\n",
    "            fo_spec.iloc[:,i] = fo_spec.iloc[:,i-1]\n",
    "        else:    \n",
    "            fo_spec.iloc[:,i] = (spectra.iloc[:,i+1]- spectra.iloc[:,i])\n",
    "        \n",
    "    #fo_spec = 100*fo_spec\n",
    "    return(fo_spec.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a34a60",
   "metadata": {},
   "source": [
    "## Continuum Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ffaddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuum_removal(points, show=False):\n",
    "    x1, y1 = points.T\n",
    "    augmented = np.concatenate([points, [(x1[0], np.min(y1)-1), (x1[-1], np.min(y1)-1)]], axis=0)\n",
    "    hull = ConvexHull(augmented)\n",
    "    continuum_points = points[np.sort([v for v in hull.vertices if v < len(points)])]\n",
    "    continuum_function = interp1d(*continuum_points.T)\n",
    "    yprime = continuum_function(x1) - y1\n",
    "    #yprime = y1 / continuum_function(x1)\n",
    "\n",
    "    if show:\n",
    "        fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "        axes[0].plot(x1, y1, label='Data')\n",
    "        axes[0].plot(*continuum_points.T, label='Continuum')\n",
    "        axes[0].legend()\n",
    "        axes[1].plot(x1, yprime, label='Data / Continuum')\n",
    "        axes[1].legend()\n",
    "\n",
    "    return yprime\n",
    "\n",
    "\n",
    "def continuum_removed(spectra):\n",
    "    cr_spec = spectra.copy()    \n",
    "    row, col = spectra.shape\n",
    "    x1 = np.arange (0, col, 1)\n",
    "    \n",
    "    \n",
    "    for r in range(0,row,1):\n",
    "        y1 = cr_spec.iloc[r,:]\n",
    "        points = np.c_[x1, y1]\n",
    "        yprime = continuum_removal(points, show=False)\n",
    "        cr_spec.iloc[r,:] = yprime\n",
    "        \n",
    "    return cr_spec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40ff2f",
   "metadata": {},
   "source": [
    "## Resampling (n_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28824f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_spectra (spectra, n_band):\n",
    "    row, width = spectra.shape\n",
    "    if n_band == 0:\n",
    "        red_spectra = spectra.copy()\n",
    "    else:\n",
    "        w = width/n_band\n",
    "        \n",
    "        #----- obtaining the sampling locations in indx----\n",
    "        indx = []\n",
    "        for i in range (0,n_band,1):\n",
    "            indx.append(np.floor((i+0.5)*w))\n",
    "            \n",
    "        #------ applying smoothing filter on spectra---------\n",
    "        temp_smooth = sgsmooth (spectra, np.floor(0.5*w).astype(int))\n",
    "        \n",
    "        #------ picking values at sampling locations---------\n",
    "        red_spectra = temp_smooth.iloc[:, indx].copy()\n",
    "        \n",
    "    return (red_spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8681e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- adaptive binning of 'spectra' in 'n_band' using different 'score' for each wavelengths-----\n",
    "def ad_resample_spectra (spectra, n_band, score):\n",
    "    row, width = spectra.shape\n",
    "    \n",
    "#     std_score = 10*(spectra.std()/max(spectra.std()))\n",
    "    \n",
    "#     r_val, p_val = find_rpval (spectra, T[0])\n",
    "#     print('r_val: ', r_val.iloc[0,:])\n",
    "#     r_val.replace(np.nan, 0, inplace=True)\n",
    "#     cor_score = abs(10*r_val.iloc[0,:].copy())\n",
    "#     corr_score = 10*(cor_score/max(cor_score))\n",
    "    \n",
    "#     wave_score = std_score + corr_score\n",
    "    \n",
    "    wave_score = score\n",
    "    \n",
    "    print(wave_score)\n",
    "    \n",
    "    sum_score = wave_score.sum()\n",
    "    band_score = sum_score/n_band\n",
    "    print('sum_score:', sum_score, '----> n_band:',n_band, '-----> band_score', band_score)     \n",
    "    \n",
    "    if n_band == 0:\n",
    "        red_spectra = spectra.copy()\n",
    "    else:\n",
    "        #----- finding boundaries of bins--------------\n",
    "        cur_score = 0\n",
    "        prev_score = 0\n",
    "        boundary = [0]\n",
    "        print('initial boundary', boundary)\n",
    "        for j in range (0, width):\n",
    "            l = len(boundary)\n",
    "            cur_score = prev_score + wave_score.iloc[j] \n",
    "            print('current boundary at j=',j, boundary)\n",
    "            print('cur_score:', cur_score, 'l*band_score:', l*band_score, 'prev_score:', prev_score)\n",
    "            if (cur_score >= (l*band_score) and prev_score < (l*band_score)):\n",
    "                boundary.append(j)\n",
    "                print('updated boundary')\n",
    "                     \n",
    "            prev_score = cur_score\n",
    "            \n",
    "        boundary.append(width-1)\n",
    "        print('final boundary', boundary)\n",
    "        \n",
    "        #---- obtaining the sampling locations (mid points) in indx----\n",
    "        indx = []\n",
    "        for i in range (0,n_band,1):\n",
    "            print('i+1 is:', i+1)\n",
    "            indx.append(int(0.5*(boundary[i]+ boundary[i+1])))\n",
    "        \n",
    "        #---- initializing the output (resampled) spectra----\n",
    "        red_spectra = spectra.iloc[:, indx].copy()\n",
    "        \n",
    "        \n",
    "        #---- finalizing the value at the sampled locations---\n",
    "        for c in range (0, len(indx)):\n",
    "            min_c = boundary[c]\n",
    "            max_c = boundary[c+1]\n",
    "            red_spectra.iloc[:, c] = spectra.iloc[:, min_c:max_c].mean(axis=1).copy()            \n",
    "       \n",
    "    return  red_spectra "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65096a1",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing (Target Variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50a21b",
   "metadata": {},
   "source": [
    "## Z-score Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d61e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score (target):\n",
    "    X = target.copy()\n",
    "    # outliers removal\n",
    "    mean = X.mean()\n",
    "    std = X.std()\n",
    "    X.loc[abs((X - mean)) >= 4*std] = mean\n",
    "    \n",
    "    X = (X-mean)/std\n",
    "    return (X.copy())\n",
    " \n",
    "\n",
    "# udf = pd.read_csv('uae.csv')\n",
    "# Y = lognormal (udf['TOC'].copy())\n",
    "# plt.hist(udf['TOC']/2.5, bins=98)\n",
    "# plt.show()\n",
    "# plt.hist(Y, bins=98)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee5dff0",
   "metadata": {},
   "source": [
    "## Standard Normalization (Log+MinMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f99f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normal (target):\n",
    "    X = target.copy()\n",
    "    # outliers removal\n",
    "    mean = X.mean()\n",
    "    std = X.std()\n",
    "    X.loc[abs((X - mean)) >= 4*std] = mean\n",
    "    # shift min to 1\n",
    "    m = X.min()\n",
    "    dis = (1-m)\n",
    "    X = X + dis    \n",
    "    # apply log transform\n",
    "    #X = np.log(X)    \n",
    "    # normalize/rescale using Min-Max method \n",
    "    minX = X.min()\n",
    "    maxX = X.max()\n",
    "    diff = maxX - minX\n",
    "    X = ((X-minX)*10)/diff    \n",
    "    return (X.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff3c14",
   "metadata": {},
   "source": [
    "## Calculating Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33fe44cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson corelation between different wavelengths and Targets/Outputs (i.e, sand, clay, silt, and TOC) \n",
    "\n",
    "def find_rpval (spectra, tar):\n",
    "    (r, c) = spectra.shape\n",
    "    \n",
    "    r_val = spectra.iloc[[0], :].copy()\n",
    "    p_val = spectra.iloc[[0], :].copy()\n",
    "    \n",
    "    for j in range(0, c):\n",
    "        r_val.iloc[0,j], p_val.iloc[0,j] = stats.pearsonr(tar, spectra.iloc[:, j])\n",
    "    \n",
    "    return(r_val, p_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8731b2",
   "metadata": {},
   "source": [
    "# 3. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "590d16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_split finds random state and minimum error for best train test split\n",
    "\n",
    "def best_split (X,y,tst_siz):\n",
    "    ymin = y.min()\n",
    "    ymax = y.max()\n",
    "    trn_siz = 1-tst_siz\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= tst_siz, random_state=0)\n",
    "    rand_st = 0\n",
    "    bin_train = np.histogram(y_train, bins = 8, range = (ymin,ymax), density=False)\n",
    "    bin_test = np.histogram(y_test, bins = 8, range = (ymin,ymax), density=False)\n",
    "    error = abs((bin_train[0])/trn_siz - (bin_test[0])/tst_siz)\n",
    "    cum_error = error.sum()\n",
    "    min_err= cum_error\n",
    "    \n",
    "    for i in np.arange(1,42,1):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= tst_siz, random_state=i)\n",
    "        bin_train = np.histogram(y_train, bins = 8, range = (ymin,ymax), density=False)\n",
    "        bin_test = np.histogram(y_test, bins = 8, range = (ymin,ymax), density=False)\n",
    "        error = abs((bin_train[0])/trn_siz - (bin_test[0])/tst_siz)\n",
    "        cum_error = error.sum()\n",
    "        if cum_error < min_err:\n",
    "            min_err = cum_error\n",
    "            rand_st = i\n",
    "            #print(i)\n",
    "\n",
    "    return (rand_st, min_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36c4380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## Plotting the Distribution of Train and Test Output Data \n",
    "# plt.style.use(['science','notebook','grid'])\n",
    "\n",
    "# fig, ax = plt.subplots(2,2, figsize=(18,14))\n",
    "\n",
    "# minSand = np.min(y_sand)\n",
    "# maxSand = np.max(y_sand)\n",
    "\n",
    "# binsSand = np.linspace(minSand, maxSand, 8)\n",
    "# # density =True : used to normalise bin heights to make the integral of  histogram 1.\n",
    "# ax[0][0].hist([ySand_train, ySand_test], binsSand , label=['Train', 'Test'], density=True, color = ['blue','red'])\n",
    "# # results in error when yN_train/ yN_test is data frame or ndarray\n",
    "# ax[0][0].legend(loc='upper left', fontsize =12)\n",
    "# ax[0][0].set_xlabel('Sand content',fontsize =16)\n",
    "# ax[0][0].set_ylabel('Normalised frequency',fontsize =12)\n",
    "# ax[0][0].tick_params(axis='both', labelsize=8)\n",
    "\n",
    "# minSilt = np.min(y_silt)\n",
    "# maxSilt = np.max(y_silt)\n",
    "\n",
    "# binsSilt = np.linspace(minSilt, maxSilt, 8)\n",
    "# # density =True : used to normalise bin heights to make the integral of  histogram 1.\n",
    "# ax[0][1].hist([ySilt_train, ySilt_test], binsSilt , label=['Train', 'Test'], density=True, color = ['blue','red'])\n",
    "# # results in error when yN_train/ yN_test is data frame or ndarray\n",
    "# ax[0][1].legend(loc='upper right', fontsize =12)\n",
    "# ax[0][1].set_xlabel('Silt content',fontsize =16)\n",
    "# ax[0][1].set_ylabel('Normalised frequency',fontsize =12)\n",
    "# ax[0][1].tick_params(axis='both', labelsize=8)\n",
    "\n",
    "# minClay = np.min(y_clay)\n",
    "# maxClay = np.max(y_clay)\n",
    "\n",
    "# binsClay = np.linspace(minClay, maxClay, 8)\n",
    "# # density =True : used to normalise bin heights to make the integral of  histogram 1.\n",
    "# ax[1][1].hist([yClay_train, yClay_test], binsClay , label=['Train', 'Test'], density=True, color = ['blue','red'])\n",
    "# # results in error when yN_train/ yN_test is data frame or ndarray\n",
    "# ax[1][1].legend(loc='upper right', fontsize =12)\n",
    "# ax[1][1].set_xlabel('Clay content',fontsize =16)\n",
    "# ax[1][1].set_ylabel('Normalised frequency',fontsize =12)\n",
    "# ax[1][1].tick_params(axis='both', labelsize=8)\n",
    "\n",
    "# minTOC = np.min(y_toc)\n",
    "# maxTOC = np.max(y_toc)\n",
    "\n",
    "# binsTOC = np.linspace(minTOC, maxTOC, 8)\n",
    "# # density =True : used to normalise bin heights to make the integral of  histogram 1.\n",
    "# ax[1][0].hist([yTOC_train, yTOC_test], binsTOC , label=['Train', 'Test'], density=True, color = ['blue','red'])\n",
    "# # results in error when yN_train/ yN_test is data frame or ndarray\n",
    "# ax[1][0].legend(loc='upper right', fontsize =12)\n",
    "# ax[1][0].set_xlabel('Total Organic Content',fontsize =16)\n",
    "# ax[1][0].set_ylabel('Normalised frequency',fontsize =12)\n",
    "# ax[1][0].tick_params(axis='both', labelsize=8)\n",
    "\n",
    "# fig.suptitle('Train Test Distribution of Data', x = 0.5 ,y = .95, fontsize = 20)\n",
    "\n",
    "# #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf30629",
   "metadata": {},
   "source": [
    "# 4. IQRP, RPD, R2, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af288039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_iqrp (Yp, Y):\n",
    "    mse = mean_squared_error(Yp, Y)\n",
    "    rmse = np.sqrt(mse)\n",
    "    X = Y.copy()\n",
    "    l = len(X)\n",
    "    q1 = math.floor(l/4)\n",
    "    q3 = math.floor(3*l/4)\n",
    "    X = X.sort_values().reset_index(drop=True)\n",
    "    res = (X[q3] - X[q1])/rmse\n",
    "    return(res)\n",
    "\n",
    "def find_rpd (Yp, Y):\n",
    "    mse = mean_squared_error(Yp, Y)\n",
    "    rmse = np.sqrt(mse)\n",
    "    res = Y.std()/rmse\n",
    "    return(res)\n",
    "\n",
    "def find_r2 (Yp, Y):\n",
    "    res = r2_score(Y, Yp)\n",
    "    return(res)\n",
    "\n",
    "def find_rmse(Yp, Y):\n",
    "    res = np.sqrt(mean_squared_error(Y, Yp))\n",
    "    return(res)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1dc0ed",
   "metadata": {},
   "source": [
    "# Parameters for Best Model Fit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da09ef",
   "metadata": {},
   "source": [
    "## PLSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c2c350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_param_PLSR (X, y, rand_n, max_n_comp):       \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=rand_n)\n",
    "    iqrpL = []\n",
    "    \n",
    "    for n in range(1,max_n_comp):\n",
    "        Model = PLSRegression(n_components=n, scale=True)\n",
    "        Model.fit(X_train, y_train)\n",
    "        y_pred = Model.predict(X_test, copy=True)        \n",
    "        iqrp_test = find_iqrp(y_pred, y_test)\n",
    "        iqrpL.append(iqrp_test)                \n",
    "    \n",
    "    IQRP = max(iqrpL)\n",
    "    n_iqrp = iqrpL.index(max(iqrpL))+1\n",
    "    \n",
    "    #print('IQRP :', IQRP,  '>>> n_comp: ', n_iqrp)    \n",
    "    return (n_iqrp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f321624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde91a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
